# -*- coding: utf-8 -*-
"""MadData24.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18GmiKVlCLoJFO8zlDr5S9408VIh_w7LO
"""

import pandas as pd

url = "Police_Incident_Reports.csv"
crime_data = pd.read_csv(url)
crime_data = crime_data.head(600)

columns_to_drop = ['CaseNumber', 'Victim', 'Suspect', 'Arrested', 'Details', 'ReleasedBy', 'DateModified']
crime_data = crime_data.drop(columns=columns_to_drop)
crime_data.dropna()
crime_data.head()
crime_data.shape



# Commented out IPython magic to ensure Python compatibility.
import datetime as dt
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_style("whitegrid")
import warnings
warnings.filterwarnings('ignore')

# from IPython.core.display import display, HTML
# display(HTML("<style>.container { width:100% !important; }</style>"))

# sns.set_palette("husl")
# sns.pairplot(crime_data, hue="IncidentType", height=10)

incident_counts = crime_data['IncidentType'].value_counts()
plt.figure(figsize=(10, 10))
plt.pie(incident_counts, labels=incident_counts.index, autopct='%1.1f%%', startangle=90)
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

import pandas as pd
import matplotlib.pyplot as plt

# Load the dataset
file_path = 'Police_Incident_Reports.csv'
crime_data = pd.read_csv(file_path)

# Assuming incident_counts is the result of crime_data['IncidentType'].value_counts()
incident_counts = crime_data['IncidentType'].value_counts()

# Replace incident types with less than 1% with 'Miscellaneous'
threshold = 0.01 * incident_counts.sum()  # 1% of the total number of incidents
incident_counts_clipped = incident_counts[incident_counts >= threshold]
incident_counts_misc = incident_counts[incident_counts < threshold].sum()
if incident_counts_misc > 0:
    incident_counts_clipped['Miscellaneous'] = incident_counts_misc

# Custom pie chart function
def custom_pie_chart(data, labels, autopct='%1.1f%%', startangle=90, save_path="/content/"):
    # Define custom colors
    colors = plt.cm.Paired.colors

    # Explode the slice with the highest count
    explode = [0.1 if count == max(data) else 0 for count in data]

    fig, ax = plt.subplots(figsize=(10, 10))
    wedges, texts, autotexts = ax.pie(data, labels=labels, autopct=autopct, startangle=startangle)

    # Ensure that pie is drawn as a circle.
    ax.axis('equal')
    plt.title('Distribution of Incident Types')

    # Save the figure if save_path is provided
    if save_path:
        plt.savefig(save_path, bbox_inches='tight')

    plt.show()

# Plot the custom pie chart
custom_pie_chart(incident_counts_clipped.values, incident_counts_clipped.index, save_path='incident_types_pie_chart.png')

"""# Latitude Longitude Generation from Address

"""

from geopy.geocoders import Nominatim

def get_lat_long(location):
    try:
        geolocator = Nominatim(user_agent="our_app")
        location = geolocator.geocode(location)
        if location:
            return location.latitude, location.longitude
        else:
            return None, None
    except Exception as e:
        print(f"Error geocoding address '{location}': {e}")
        return None, None

# # Example usage
# location = "500 block West Doty Street, Madison, WI"
# coordinates = get_lat_long(location)

# if coordinates:
#     print(f"Latitude: {coordinates[0]}, Longitude: {coordinates[1]}")
# else:
#     print(f"Unable to find coordinates for {location}")

# Fetches latitude and longitude of device

from geopy.geocoders import GoogleV3

def get_current_location():
    g = GoogleV3(api_key='AIzaSyDORt-lCJ_4f06_ccxNGfL6NOKhADFVGgc')
    location = g.reverse((43.0731, -89.4012), exactly_one=True)
    return location.latitude, location.longitude

latitude, longitude = get_current_location()

print("Latitude:", latitude)
print("Longitude:", longitude)

"""# Add Latitude and Longitude to the Data"""

crime_data['Address'] = crime_data['Address'] + ', Madison, WI'

# Apply the function to each row in the DataFrame
crime_data[['Latitude', 'Longitude']] = crime_data['Address'].apply(lambda x: pd.Series(get_lat_long(x)))

# Display the updated DataFrame
print(crime_data)

# Create new columns for Latitude and Longitude
crime_data['Latitude'] = None
crime_data['Longitude'] = None

# Loop through each row and geocode
for index, row in crime_data.iterrows():
    address = row['Address']
    print(str(index) + ", ", address)
    latitude, longitude = get_lat_long(address)
    crime_data.at[index, 'Latitude'] = latitude
    crime_data.at[index, 'Longitude'] = longitude
    print(latitude, ", ", longitude)

crime_data = crime_data.dropna()

crime_data.to_csv('crime_lat_long.csv', index=False)

from IPython.display import FileLink

FileLink(r'/content/crime_lat_long.csv')

import folium
import seaborn as sns
import matplotlib.pyplot as plt

# Create a base map of Madison
madison_map = folium.Map(location=[crime_data['Latitude'].mean(), crime_data['Longitude'].mean()], zoom_start=13)

# Add crime locations to the map
for index, row in crime_data.iterrows():
    folium.CircleMarker(
        location=[row['Latitude'], row['Longitude']],
        radius=3,
        color='blue',
        fill=True,
        fill_color='blue',
        fill_opacity=0.6,
    ).add_to(madison_map)

# Create a heatmap using seaborn within a FacetGrid
plt.figure(figsize=(10, 8))
g = sns.FacetGrid(crime_data, height=8)
g.map(sns.kdeplot, 'Latitude', 'Longitude', cmap='YlOrRd', fill=True, alpha=0.6)
plt.title('Crime Heatmap in Madison')
plt.show()

# Display the map
madison_map

import folium
import pandas as pd
from folium.plugins import HeatMap

# Create a base map of Madison
madison_map = folium.Map(location=[crime_data['Latitude'].mean(), crime_data['Longitude'].mean()], zoom_start=13)

# Calculate crime density for each zone
crime_density = crime_data.groupby(['Latitude', 'Longitude']).size().reset_index(name='CrimeCount')
max_crime_count = crime_density['CrimeCount'].max()

# Function to categorize crime density into zones
def get_zone_color(crime_count):
    if crime_count >= max_crime_count * 0.8:
        return 'red'  # High crime density
    elif crime_count >= max_crime_count * 0.5:
        return 'yellow'  # Moderate crime density
    elif crime_count >= max_crime_count * 0.1:
        return 'blue'
    else:
        return 'green'  # Low crime density

# Add Circle Markers for each zone based on crime density
for index, row in crime_density.iterrows():
    folium.CircleMarker(
        location=[row['Latitude'], row['Longitude']],
        radius=row['CrimeCount'] / max_crime_count * 30,  # Adjust the radius based on crime count
        color=get_zone_color(row['CrimeCount']),
        fill=True,
        fill_color=get_zone_color(row['CrimeCount']),
        fill_opacity=0.7,
    ).add_to(madison_map)

# Display the map
madison_map

import folium
import pandas as pd

# Create a base map of Madison
madison_map = folium.Map(location=[crime_data['Latitude'].mean(), crime_data['Longitude'].mean()], zoom_start=13)

# Calculate crime density for each zone
crime_density = crime_data.groupby(['Latitude', 'Longitude']).size().reset_index(name='CrimeCount')
max_crime_count = crime_density['CrimeCount'].max()

# Function to categorize crime density into zones
def get_zone_color(crime_count):
    if crime_count >= max_crime_count * 0.8:
        return 'red'  # High crime density
    elif crime_count >= max_crime_count * 0.5:
        return 'yellow'  # Moderate crime density
    # else:
    #     return 'green'  # Low crime density

# Add Circle Markers for each zone based on crime density
for index, row in crime_density.iterrows():
    folium.CircleMarker(
        location=[row['Latitude'], row['Longitude']],
        radius=row['CrimeCount'] / max_crime_count * 30,  # Adjust the radius based on crime count
        color=get_zone_color(row['CrimeCount']),
        fill=True,
        fill_color=get_zone_color(row['CrimeCount']),
        fill_opacity=0.7,
    ).add_to(madison_map)

# Display the map
madison_map

import geopandas as gpd
import matplotlib.pyplot as plt

# Load the Madison map shapefile or GeoJSON file
madison_map = gpd.read_file('/content/madison_map.geojson')

# Calculate overall crime density for the entire area
total_crime_count = len(crime_data)

# Function to categorize overall crime density into zones
def get_map_color(crime_count):
    if crime_count >= total_crime_count * 0.8:
        return 'red'  # High overall crime density
    elif crime_count >= total_crime_count * 0.5:
        return 'yellow'  # Moderate overall crime density
    else:
        return 'green'  # Low overall crime density

# Add a new column to the Madison map GeoDataFrame with the color based on overall crime density
madison_map['color'] = madison_map['geometry'].apply(lambda x: get_map_color(total_crime_count))

# Plot the colored map
fig, ax = plt.subplots(figsize=(10, 10))
madison_map.plot(ax=ax, color=madison_map['color'], edgecolor='black', linewidth=1)
plt.title('Madison Map Colored by Overall Crime Density')
plt.axis('off')  # Turn off the axis
plt.show()

import geocoder

def get_current_gps_coordinates():
    g = geocoder.ip('me')  # this function is used to find the current information using our IP Address
    if g.latlng is not None:  # g.latlng tells if the coordinates are found or not
        return g.latlng
    else:
        return None

# Call the function to get the current coordinates
coordinates = get_current_gps_coordinates()

if coordinates is not None:
    latitude, longitude = coordinates
    print("Your current GPS coordinates are:")
    print(f"Latitude: {latitude}")
    print(f"Longitude: {longitude}")
else:
    print("Unable to retrieve your GPS coordinates.")

"""# Adding features - Time Related"""

# Commented out IPython magic to ensure Python compatibility.
#### Importing Libraries and Modules
import pandas as pd
import numpy as np
import datetime as dt
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set_style("whitegrid")
import warnings
warnings.filterwarnings('ignore')

crime_data['incident_datetime'] = pd.to_datetime(crime_data['IncidentDate'], errors='coerce')
crime_data['weekday'] = crime_data['incident_datetime'].dt.dayofweek
crime_data['weekofyear'] = crime_data['incident_datetime'].dt.weekofyear
crime_data['hourofday'] = crime_data['incident_datetime'].dt.hour
crime_data["workhour"] = crime_data["hourofday"].map(lambda x: 1 if x in range(9,17) else 0)
crime_data["sunlight"] = crime_data["hourofday"].map(lambda x: 1 if x in range(7,19) else 0)

crime_data.head()

"""# Adding features - Location related"""

from sklearn.cluster import MiniBatchKMeans
### KNN cluster for lat and long
coords = np.vstack((crime_data[['Latitude', 'Longitude']].values))
sample_ind = np.random.permutation(len(coords))[:500000]
kmeans = MiniBatchKMeans(n_clusters=33, batch_size=10000).fit(coords[sample_ind])

### Applying cluster on training data
kmeans.predict(crime_data[['Latitude', 'Longitude']])
crime_data.loc[:, 'clustertype'] = kmeans.predict(crime_data[['Latitude', 'Longitude']])

### Visualizing the clusters

sns.set_style("whitegrid")
## Scatterplot of the crimes
sns.lmplot(x="Latitude", y="Longitude",data = crime_data[crime_data['Latitude']!=0.0],fit_reg=False,hue='clustertype')

#Seaborn FacetGrid, split by crime Category
g= sns.FacetGrid(crime_data[crime_data['Latitude']!=0.0], col="IncidentType", col_wrap=6)
#Kernel Density Estimate plot
g.map(sns.kdeplot, "Latitude", "Longitude", n_levels =20)

## Making a temp data frame with just category and clusters with Date as index"
df = crime_data.copy()
df['incident_datetime'] = pd.to_datetime(df['incident_datetime'], errors='coerce')
df['Date']= df['incident_datetime'].dt.date
df = df[['IncidentType','clustertype','Date']]
df['IncidentType'] = df['IncidentType'].astype('category')

f_df = pd.DataFrame()


crimes = list(df['IncidentType'].unique())
for c in crimes:
#     print (c)
    pivot_df = pd.DataFrame()
    temp = df[df['IncidentType']==c]
    pivot_df= pd.pivot_table(temp,index=["Date"],values=["clustertype"],
               columns=["clustertype"],aggfunc=[len],fill_value=0).reset_index()
    res ={}
    flattened = pd.DataFrame(pivot_df.to_records())
    for col in flattened.columns[2:]:
        cluster = int(col.split(",")[2].replace(")","").replace(" ","").lstrip())
        res[cluster] =sum(flattened[col])
        res['Crime_Type'] = c

    res_df = pd.DataFrame(res,index=[0])
    f_df = f_df.append(res_df)
f_df = f_df.set_index('Crime_Type')
f_df['Total'] = f_df.sum(axis=1)
f_df

def crime_in_cluster_till_date(crime_date,crime,cluster):

    temp = crime_data[(crime_data['IncidentType']==crime)&(crime_data['incident_datetime']<=crime_date)]

    num = temp[temp['clustertype']==cluster]['clustertype'].count()
    den = temp['clustertype'].count()
    try:
        return float(num/den)
    except:
        return np.nan

crimes = list(df['IncidentType'].unique())
for crime in crimes:
    print ("processing " + crime)
    crimecol = str('crime_rate_')+crime
    crime_data[crimecol]=None
    for index, row in crime_data.iterrows():
        crime_date = row['incident_datetime']
        cluster = row['clustertype']
        res = crime_in_cluster_till_date(crime_date,crime,cluster)
        crime_data.loc[index,crimecol] = res*100

crime_data = crime_data.fillna(0)

"""# Count crimes time wise"""

from datetime import timedelta
## count the number of crimes for the lag time
crime_data['incident_datetime'] = pd.to_datetime(crime_data['incident_datetime'], errors='coerce')
def count_agg_lag(row,lag):
    crime_time = row['incident_datetime']
    crime_lag_hours = row['incident_datetime'] - timedelta(hours = lag)
    cluster = row['clustertype']
    ## construct a dataframe with incidents between the current time and the lag time in the same cluster
    count_crimes = crime_data[(crime_data.incident_datetime > crime_lag_hours) & (crime_data.incident_datetime < crime_time) & \
                        (crime_data.clustertype == cluster)].shape[0]
    return(count_crimes)

crime_data['count_crimes_lag_1'] = crime_data[['incident_datetime','clustertype']].apply(lambda x: count_agg_lag(x,24), axis = 1)
crime_data['count_crimes_lag_7'] = crime_data[['incident_datetime','clustertype']].apply(lambda x: count_agg_lag(x,168), axis = 1)

crime_data.head()

from datetime import timedelta

# Convert 'incident_datetime' to datetime if not already
crime_data['incident_datetime'] = pd.to_datetime(crime_data['incident_datetime'], errors='coerce')

# Find the most recent date in the 'incident_datetime' column
most_recent_date = crime_data['incident_datetime'].max()

most_recent_date

def count_agg_lag(row, target_date):
    crime_time = row['incident_datetime']
    crime_lag_hours = row['incident_datetime'] - timedelta(hours=24)
    cluster = row['clustertype']

    # Filter incidents between the target date and the current time in the same cluster
    count_crimes = crime_data[(crime_data.incident_datetime > target_date) & (crime_data.incident_datetime < crime_time) & (crime_data.clustertype == cluster)].shape[0]

    return count_crimes

# Apply the function to calculate lag features based on the target date
crime_data['count_crimes_lag_1'] = crime_data.apply(lambda x: count_agg_lag(x, most_recent_date - timedelta(days=1)), axis=1)
crime_data['count_crimes_lag_7'] = crime_data.apply(lambda x: count_agg_lag(x, most_recent_date - timedelta(days=7)), axis=1)

crime_data.head()

most_recent_date = crime_data['incident_datetime'].max()

print(most_recent_date)

from datetime import timedelta

def check_crimes_in_same_cluster(row, lag, crime_category):
    crime_time = row['incident_datetime']
    crime_lag_hours = row['incident_datetime'] - timedelta(hours=lag)
    cluster = row['clustertype']
    count_crimes = crime_data[
        (crime_data.incident_datetime > crime_lag_hours) & (crime_data.incident_datetime < crime_time) &
        (crime_data.clustertype == cluster) & (crime_data.IncidentType.isin(crime_category))
    ].shape[0]

    if count_crimes > 0:
        return 1
    else:
        return 0

crime_categories_to_include = [
    'Murder/Homicide', 'Sexual Assault', 'Weapons Violation', 'Traffic Incident', 'Suspicious Person',
    'Non-Residential Burglary', 'Residential Burglary', 'Robbery', 'Theft', 'Intoxicated/Impaired Driver',
    'Disturbance', 'Battery', 'Injured Person', 'Arrested Person', 'Attempted Homicide', 'Information',
    'Traffic incident/Road Rage', 'Miscellaneous Sex Offense', 'Suspicious Vehicle', 'Damaged Property',
    'Fraud', 'Drug Investigation', 'Exposure', 'Threats', 'Check Person', 'Missing Adult',
    'Animal Complaint - Disturbance', 'Death Investigation', 'Missing Juvenile'
]

for crime_category in crime_categories_to_include:
    column_name = f'has_{crime_category.replace("/", "_").replace(" ", "_")}_lag_7'
    crime_data[column_name] = crime_data[['incident_datetime', 'clustertype']].apply(
        lambda x: check_crimes_in_same_cluster(x, 168, [crime_category]), axis=1
    )

crime_data.head()

crime_data.to_csv('/content/train_with_features.csv')

crime_data.head()

# Assuming the crime rates are in percentage format, scale them to be between 0 and 1
crime_data[['crime_rate_Murder/Homicide', 'crime_rate_Sexual Assault', 'crime_rate_Weapons Violation', 'crime_rate_Traffic Incident', 'crime_rate_Suspicious Person', 'crime_rate_Non-Residential Burglary', 'crime_rate_Residential Burglary']] /= 100

# Aggregate crime rates across different types
crime_data['total_crime_rate'] = crime_data[['crime_rate_Murder/Homicide', 'crime_rate_Sexual Assault', 'crime_rate_Weapons Violation', 'crime_rate_Traffic Incident', 'crime_rate_Suspicious Person', 'crime_rate_Non-Residential Burglary', 'crime_rate_Residential Burglary']].sum(axis=1)

# Define thresholds for overall crime rate
threshold1 = 0.5
threshold2 = 0.8

# Create labels
crime_data['safety_level'] = pd.cut(crime_data['total_crime_rate'], bins=[-float('inf'), threshold1, threshold2, float('inf')], labels=['green', 'yellow', 'red'])

crime_data.head()

from datetime import timedelta

def check_crimes_in_same_cluster(row, lag, crime_category):
    crime_time = row['incident_datetime']
    crime_lag_hours = row['incident_datetime'] - timedelta(hours=lag)
    cluster = row['clustertype']

    # Count the occurrences of the specific crime category within the same cluster and time window
    count_crimes = crime_data[
        (crime_data.incident_datetime > crime_lag_hours) & (crime_data.incident_datetime < crime_time) &
        (crime_data.clustertype == cluster) & (crime_data.IncidentType == crime_category)
    ].shape[0]

    # Count the total occurrences of all crimes within the same cluster and time window
    total_crimes = crime_data[
        (crime_data.incident_datetime > crime_lag_hours) & (crime_data.incident_datetime < crime_time) &
        (crime_data.clustertype == cluster)
    ].shape[0]

    # Calculate the crime rate as the ratio of the specific crime category to the total crimes
    crime_rate = count_crimes / total_crimes if total_crimes > 0 else 0

    return crime_rate

crime_categories_to_include = [
    'Murder/Homicide', 'Sexual Assault', 'Weapons Violation', 'Traffic Incident', 'Suspicious Person',
    'Non-Residential Burglary', 'Residential Burglary', 'Robbery', 'Theft', 'Intoxicated/Impaired Driver',
    'Disturbance', 'Battery', 'Injured Person', 'Arrested Person', 'Attempted Homicide', 'Information',
    'Traffic incident/Road Rage', 'Miscellaneous Sex Offense', 'Suspicious Vehicle', 'Damaged Property',
    'Fraud', 'Drug Investigation', 'Exposure', 'Threats', 'Check Person', 'Missing Adult',
    'Animal Complaint - Disturbance', 'Death Investigation', 'Missing Juvenile'
]

for crime_category in crime_categories_to_include:
    column_name = f'crime_rate_{crime_category.replace("/", "_").replace(" ", "_")}_lag_7'
    crime_data[column_name] = crime_data[['incident_datetime', 'clustertype']].apply(
        lambda x: check_crimes_in_same_cluster(x, 168, crime_category), axis=1
    )

# Aggregate crime rates across different types
crime_data['total_crime_rate'] = crime_data[[f'crime_rate_{crime_category.replace("/", "_").replace(" ", "_")}_lag_7' for crime_category in crime_categories_to_include]].sum(axis=1)

# Define thresholds for overall crime rate
threshold1 = 0.5
threshold2 = 0.8

# Create labels
crime_data['safety_level'] = pd.cut(crime_data['total_crime_rate'], bins=[-float('inf'), threshold1, threshold2, float('inf')], labels=['green', 'yellow', 'red'])

crime_data.to_csv('/content/train_with_features.csv')

crime_data.head()

cluster_safety_levels = {}

for cluster in crime_data['clustertype'].unique():
    # Calculate the average crime rate (or any other metric) for each cluster
    avg_crime_rate = crime_data[crime_data['clustertype'] == cluster]['IncidentType'].mean()

    # Determine the safety level based on the average crime rate
    if avg_crime_rate < 0.1:  # Adjust the threshold_value as needed
        safety_level = 'Green'
    elif avg_crime_rate < 0.5:
        safety_level = 'Yellow'
    else:
        safety_level = 'Red'

    cluster_safety_levels[cluster] = safety_level

# Map safety levels back to the 'Cluster' column to create the 'safety_index' column
crime_data['safety_index'] = crime_data['Cluster'].map(cluster_safety_levels)

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt

crime_data['Time'] = pd.to_datetime(crime_data['incident_datetime'])
crime_data['Time'] = crime_data['incident_datetime'].astype(int) / 10**9  # Convert to seconds since epoch

# Encode the 'CrimeType' column
label_encoder = LabelEncoder()
crime_data['IncidentType'] = label_encoder.fit_transform(crime_data['IncidentType'])

# Select relevant features
X = crime_data[['Latitude', 'Longitude', 'Time', 'IncidentType']]

# Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine the optimal number of clusters using the Elbow Method
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

# Plot the Elbow Method graph
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')  # WCSS stands for "within-cluster sum of squares"
plt.show()

# Based on the Elbow Method, choose the optimal number of clusters and fit the K-Means model
optimal_clusters = 3  # Adjust this based on the plot
kmeans = KMeans(n_clusters=optimal_clusters, init='k-means++', max_iter=300, n_init=10, random_state=42)
crime_data['Cluster'] = kmeans.fit_predict(X_scaled)

# Now, the 'Cluster' column in crime_data represents the grouping of areas based on crime patterns

crime_data.head()

cluster_safety_levels = {}

for cluster in crime_data['Cluster'].unique():
    # Calculate the average crime rate (or any other metric) for each cluster
    avg_crime_rate = crime_data[crime_data['Cluster'] == cluster]['IncidentType'].mean()

    # Determine the safety level based on the average crime rate
    if avg_crime_rate < 0.1:  # Adjust the threshold_value as needed
        safety_level = 'Green'
    elif avg_crime_rate < 0.5:
        safety_level = 'Yellow'
    else:
        safety_level = 'Red'

    cluster_safety_levels[cluster] = safety_level

# Map safety levels back to the 'Cluster' column to create the 'safety_index' column
crime_data['safety_index'] = crime_data['Cluster'].map(cluster_safety_levels)

crime_data.head()